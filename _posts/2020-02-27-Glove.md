---
layout: post
title:  "nlp_GloVe"
description: natural language processing.
date:   2020-02-27
use_math: true
---

### GloVe
natural language processing.

**LSA** : count based embedding -->  
ex) King:man = Queen: ? (answer : woman) / but weak at *Analogy task*.  
**Word2vec** : Prediction based embedding -->  
weak at reflecting the overall statistics of the Corpus.  

 1. #### Window based Co-occurrence Matrix
![word embedding](https://drive.google.com/uc?export=view&id=1wrowZoJfD0nJeq0vCiP11Kop71rDhPXk)  
![word embedding](https://drive.google.com/uc?export=view&id=1xfn7tB5bWTy_n_TrH0mAbMuSUNJ-b57h)  
---  
Window size : 1  

 2. #### Co-occurence Probability
![word embedding](https://drive.google.com/uc?export=view&id=19Tm60I075lcZpCZ2Qt9d12gp5nWI1rPc)  
---  
The probability of 'ice' appearing when 'solid' apears : 0.00019  
The probability of 'steam' appearing when 'solid' appears: 0.000022  
---  
$P(solid \mid ice) / P(solid \mid steam) = 8.9$, which means that the word 'solid' is more closely linked to the word 'ice' than the word 'steam'.

 3. #### Loss function
![word embedding](https://drive.google.com/uc?export=view&id=1P-M1Kw67qKq3NVpdHiHAy964Jv1OtIEI)  
---  
The basic idea of **GloVe** is as follows:    
$$dot product(w_{i} \tilde{w}_{k}) \approx logP(k \mid i) = log P_{ik}$$  
---  
$$F(w_{i}-w{j}, \tilde{w}_{k}) = \frac{P_{ik}}{P{jk}} --- (1)$$  
---  
However, while the right side of (1) is scalar, the left side is vector. Thus,  
---  
$$F((w_{i}-w{j})^{T}\tilde{w}_{k}) = \frac{P_{ik}}{P{jk}} --- (2)$$  
---  
But, since the center word {i} and the context word {k} can be choosed arbitrarily,  
then, the Function F(x) should be **Homomorphism**.  
---  
Homomorphism : F(a+b) = F(a)F(b) for all a,b.  
To apply Homomorphism to the equation (2),  
F(a-b) = F(a)/F(b) for all a,b.  
---  
Thus, the left side of (2) can be transformed as follows,  
---  
$$F((w_{i}-w{j})^{T}\tilde{w}_{k}) = \frac{F(w_{i}^{T}\tilde{w}_{k})}{F(w_{j}^{T}\tilde{w}_{k})} --- (3)$$  
---  
Equation (3) can be transformed as follows,  
---  
$$F(w_{i}^{T}\tilde{w}_{k}-w_{j}^{T}\tilde{w}_{k}) = \frac{F(w_{i}^{T}\tilde{w}_{k})}{F(w_{j}^{T}\tilde{w}_{k})} --- (4)$$  
---  
And, the Exponential function satisfies the equation (4).  
---  
$$exp(w_{i}^{T}\tilde{w}_{k}-w_{j}^{T}\tilde{w}_{k}) = \frac{exp(w_{i}^{T}\tilde{w}_{k})}{exp(w_{j}^{T}\tilde{w}_{k})}$$  
$$exp(w_{i}^{T}\tilde{w}_{k}) = P_{ik} = \frac{X_{ik}}{X_{i}} --- (5)$$  
---  
by (5),  
---  
$$w_{i}^{T}\tilde{w}_{k} = logP_{ik} = log(\frac{X_{ik}}{X_{i}}) = logX_{ik} - logX_{i} --- (6)$$  
---  
However, as described above, $w_{i}$ and $$\tilde{w}_{k}$$ should be exchangeable in equation (6).  
Thus, the existence $$logX_{i}$$ is replaced by $$b_{i}$$ and $$\tilde{b}_{k}$$, which are the bias of $$W_{i}$$ and $$\tilde{w}_{k}$$, respectively.  
---  
$$w^{T}_{i}\tilde{w}_{k} + b_{i} + \tilde{b}_{k} = logX_{ik}$$  
---  
$$Loss Function = \sum_{m,n=1}^{V}{(w_{m}^{T}\tilde{w}_{n} + b_{m} + \tilde{b}_{n} - logX_{mn})^{2}}$$  
---  
However, it is possible that **co-occurence matrix X** is a * [Sparse Matrix](https://en.wikipedia.org/wiki/Sparse_matrix).  
Thus, they contrived a **Weighting function**, f(x).  
![word embedding](https://drive.google.com/uc?export=view&id=1Rr0Cud0Xv0n1cSKfd3YVNwiQ7_NlDX3i)  
---  
$$f(x) = min(1, (\frac{x}{x_{max}})^{3/4})$$  
If x is small, then f(x) is also small  
If x is large, then f(x) is also large  
But If x is **too** large, then f(x) is limited by 1  
---  
To conclude,  
---  
$$Loss Function = \sum_{m,n=1}^{V}{f(X_{mn})(w_{m}^{T}\tilde{w}_{n} + b_{m} + \tilde{b}_{n} - logX_{mn})^{2}}$$  
 
### reference
* [Won Joon Yoo. 딥 러닝을 이용한 자연어 처리 입문. wikidocs.](https://wikidocs.net/22885)