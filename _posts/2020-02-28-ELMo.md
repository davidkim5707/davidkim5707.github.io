---
layout: post
title: Elmo
categories:
  - Programming
tags:
  - programming
last_modified_at: 2020-02-23
use_math: true
---


### ELMo
natural language processing.

**Pre-trained language model**

 1. #### Embeddings from Language Model
Ex) 'Bank Account' vs 'River Bank'  
---  
In *Word2Vec* and *GloVe*,  
---  
Bank = [0.2 0.8 -1.2]  
---  
However, the meaning of *bank* in 'Bank Account' and 'River Bank' is different.  
---  
To reflecting those contextualized meaning of words,  
---  
**ELMO** : Contextualized Word Embedding  

 2. #### Pre-train of biLM(Bidirectional Language Model)
![word embedding](https://drive.google.com/uc?export=view&id=1pClcXAycQBFGczimWeqNjMFB0gJY5olb)  
---  
Above image is *RNN language model**.  
---  
![word embedding](https://drive.google.com/uc?export=view&id=1PMph5DXBcbB8mC-JOFbMzAx6-2Ao0wxa)  
---  
**ELMo** is Bidirectional Language Model.  
---  
Bidirectional RNN : concatenate the both hidden layers **before** sending to the next step.  
ELMO : concatenate the both hidden layers **after** sending to next step.

 3. #### Application of biLM
![word embedding](https://drive.google.com/uc?export=view&id=1Zb-C-3RcA1sRxFo5uxaRp6fZjCXDT7aq)  
---  
Output of each layer : embedding layer  
Other layer : hidden layer
---  
Thus, ELMo uses every output getting from all layers since they may be expected to have different imformation respectively.  
---  
1) Concatenating each layers' output  
![word embedding](https://drive.google.com/uc?export=view&id=1X1icA1CN0OhbJvRztpTIzrNWloY13DWz)  
---  
2) give Weights to each layers' output  
![word embedding](https://drive.google.com/uc?export=view&id=15pdTxQebikIaVtj-Lltq99CdJyMAspvN)  
---  
3) Weighted sum of each layers' output  
![word embedding](https://drive.google.com/uc?export=view&id=1fMnczalPqKCFqFk23pUMk6lnCZ0lnyog)  
---  
4) Multiplying scalar $\gamma$ to the weighted sum
![word embedding](https://drive.google.com/uc?export=view&id=1GXWxSGaRnTTSRlwW8ajN7LXU7_P1IHgM)  
---  
This final vector is called as **ELMo representation**.  
---  
![word embedding](https://drive.google.com/uc?export=view&id=1VxHlYSqvfZqcqtF9441JREMUMmOUjYMb)  
---  
Above image shows the combination of ELMo representation and GloVe.

### reference
* [Won Joon Yoo. 딥 러닝을 이용한 자연어 처리 입문. wikidocs.](https://wikidocs.net/33930)