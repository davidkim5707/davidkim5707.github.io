---
layout: post
title: Word2Vec
categories:
  - Programming
tags:
  - programming
last_modified_at: 2020-02-23
use_math: true
---

### Word2Vec
natural language processing.

### Word Embedding
Before Entering into the explanation regarding Word2Vec, I'm going to briefly explicate "Word Embedding".

 1. #### Sparse Representation
 All vectors and matrix representing the word consist of only 0 and 1.
 This is related to [One-hot encoding](https://wikidocs.net/22647)  .  
 Ex) dog = [ 0 0 0 0 1 0 0 0 0 0 0 0 ...  ... 0]  # 10000 dimension    
 But, this kind of one-hot encoding has a disadvantage of causing "space waste".

 2. #### Dense Representation
Users can reduce the dimension of the above "sparse representation" as they want.  
Ex) dog = [0.2 1.8 1.1 -2.1 1.1 2.8 ...  ...] # 128 dimension

 3. #### Word Embedding
The way of representing a word as a "dense representation" is called **"Word Embedding"**  
![word embedding](https://drive.google.com/uc?export=view&id=1SDLELRvxvFSkiHGz1WmcDXTC7hsxPCrR)

### Word2Vec  
**Word2vec** is basically based on "distributed representation",  which is same with "dense representation" described above. 

 1. #### CBOW(Continuous Bag of Words)
CBOW is a method predicting the middle word locating at the middle of a sentence.  
---  
*Sentence = "The cat sat on the mat."  
center word = "sat"  
context word = {"The", "fat", "cat", "on", "the", "mat"}*  
---  
Thus, by using context words, the center word is predicted.  
---  
*window = a scope indicating how many words are selected back and forth of the centerword as context words.*  
---  
![window](https://drive.google.com/uc?export=view&id=1p9fF15IQUD7GbHS1x67qm2qYEZ5k_j1t)  
---  
Above image indicates a method, "sliding window", with which you can make a dataset to train as changing the choice of the center word and the context words by choosing the window as 2.  
---  
![window](https://drive.google.com/uc?export=view&id=1Tmshx6TdEC3D9lrJEs36kbTVjvWJTEFi)  
---  
This image indicates the case where the dimension of projection layer is equal to 5 and the center word is assigned to "sat".  
Since the dimension of projection layer is equal to 5, you will get *5-dimensional embedding vector* after **CBOW**.  
---  
![window](https://drive.google.com/uc?export=view&id=1--0bQUx4_8Pl-vngS3XtqhrRHGfNZKk9)  
---  
the dimension of one-hot vector : 7  
the dimension of embedding vector: 5  
W: $7\times5$ matrix  
W': $5\times7$ matrix  
---  
Thus, given the center word and context words, CBOW is relentlessly repeating the process until getting the most efficacious **matrix W and W'** which minimize the **loss funtion**.  
---  
![window](https://drive.google.com/uc?export=view&id=1mLh03TBZgDpNci4hfuoC94tuZ_zMr1JD)  
---  
$\hat{y}$ : **'Score Vector'**  
In here, CBOW uses **cross-entropy function** as their loss function.  
---  
![window](https://drive.google.com/uc?export=view&id=1XSZXcf7zQcrqXIj2yEMOQwu_3SlmUH1w)  
---  
Since the center word, y, is one-hot vector whose jth row is the unique row equal to 1, this loss function can be simplified as follows.  
---  
![window](https://drive.google.com/uc?export=view&id=13x_qjm9PG7NqxvOrRqvyY1xwck9cFu88)  
---  
If the process succeeded in predicting the center world exactly, then the above equation would be $-1log(1)=0$.  
To conclude, **CBOW** is a process,in which the matrix **W and W'** are constantly calcuated by **Back Propagating** of those aforementioned structures until the loss function is minimized, to derive *"center word"* based on the *"context words"*.  

 2. #### Skip-gram 
![window](https://drive.google.com/uc?export=view&id=1fVJieIWV4DjTB2hyGP5dfDeMYx0eEK3s)  
---  
Contrary to **CBOW**, **Skip-gram** predicts context words by using center word.  
**Skip-gramm** is known to be better that **CBOW**.  

 3. #### Negative Sampling
To reduce the useless process in **Word2Vec**, **Negative Sampling** assigns positive sign(+) to the **peripheral words** and negative sign(-) to the randomly sampling words.  
Thus, before conducting **Word2Vec**, it is needed to work on **grouping words**.  
---  
peripheral group : {'cat', 'dog', 'attractiveness'}  
randomly sampled useless group : {'computer', 'conference', 'pizza'}  
---  
By using this kind of binary classification, **Word2Vec** can be more efficacious in computational volume.  

### further learning
* [Further learning for Word2vec](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/)
### reference
* [Won Joon Yoo. 딥 러닝을 이용한 자연어 처리 입문. wikidocs.](https://wikidocs.net/22660)